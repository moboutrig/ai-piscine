{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python369jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6",
   "display_name": "Python 3.6.9 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "\n",
    "# Exercise 5 Machine Learning models\n",
    "\n",
    "The goal of this exercise is to have an overview of the existing Machine Learning models and to learn to call them from scikit learn.\n",
    "We will focus on:\n",
    "\n",
    "- SVM/ SVC\n",
    "- Decision Tree\n",
    "- Random Forest (Ensemble learning)\n",
    "- Gradient Boosting (Ensemble learning, Boosting techniques)\n",
    "\n",
    "All these algorithms exist in two versions: regression and classification. Even if the logic is similar in both classification and regression, the loss function is specific to each case.\n",
    "\n",
    "It is really easy to get lost among all the existing algorithms. This article is very useful to have a clear overview of the models and to understand which algorithm use and when. https://towardsdatascience.com/how-to-choose-the-right-machine-learning-algorithm-for-your-application-1e36c32400b9\n",
    "\n",
    "Preliminary:\n",
    "\n",
    "- Import California Housing data set and split it in a train set and a test set (10%). Fit a linear regression on the data set. *The goal is to focus on the metrics, that is why the code to fit the Linear Regression is given.*\n",
    "\n",
    "```python\n",
    "#imports\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "#data\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing['data'], housing['target']\n",
    "#split data train test \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.1,\n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=43)\n",
    "#pipeline \n",
    "pipeline = [('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('lr', LinearRegression())]\n",
    "pipe = Pipeline(pipeline)\n",
    "#fit\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "```\n",
    "\n",
    "1. Create 5 pipelines with 5 different models as final estimator (keep the imputer and scaler unchanged):\n",
    "    1. Linear Regression\n",
    "    2. SVM\n",
    "    3. Decision Tree (set `random_state=43`)\n",
    "    4. Random Forest (set `random_state=43`)\n",
    "    5. Gradient Boosting  (set `random_state=43`)\n",
    "\n",
    "Take time to have basic understanding of the role of the basic hyperparameter and their default value.\n",
    "\n",
    "- For each algorithm, print the R2, MSE and MAE on both train set and test set.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "r2 on the train set: 0.34823544284172525\nMAE on the train set: 0.5330920012614552\nMSE on the train set: 0.5273648371379568\n----------------\nr2 on the test set: 0.3551785428138904\nMAE on the test set: 0.5196420310323714\nMSE on the test set: 0.49761195027083815\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn import tree, svm\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing['data'], housing['target']\n",
    "#split data train test \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.1,\n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=43)\n",
    "#pipeline \n",
    "pipeline = [('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('lr', LinearRegression())]\n",
    "pipe = Pipeline(pipeline)\n",
    "#fit\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred_train = pipe.predict(X_train) \n",
    "y_pred_test = pipe.predict(X_test)\n",
    "\n",
    "r2_train = r2_score(y_pred_train, y_train)\n",
    "mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "mae_train = mean_absolute_error(y_train, y_pred_train)\n",
    "\n",
    "print('r2 on the train set:', r2_train)\n",
    "print('MAE on the train set:', mae_train)\n",
    "print('MSE on the train set:', mse_train)\n",
    "\n",
    "r2_test = r2_score(y_pred_test, y_test)\n",
    "mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "\n",
    "print('----------------')\n",
    "print('r2 on the test set:', r2_test)\n",
    "print('MAE on the test set:', mae_test)\n",
    "print('MSE on the test set:', mse_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "------------svm----------\n",
      "r2 on the train set: 0.6462366150965996\n",
      "MAE on the train set: 0.3835645163325987\n",
      "MSE on the train set: 0.3346447867133917\n",
      "----------------\n",
      "r2 on the test set: 0.6162644671183827\n",
      "MAE on the test set: 0.3897680598426786\n",
      "MSE on the test set: 0.3477101776543003\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('------------svm----------')\n",
    "# from sklearn import preprocessing\n",
    "\n",
    "# lab_enc = preprocessing.LabelEncoder()\n",
    "# y_encoded = lab_enc.fit_transform(y_train)\n",
    "\n",
    "# pipeline \n",
    "pipeline_svc = [('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('svr', SVR())]\n",
    "\n",
    "pipe_svc = Pipeline(pipeline_svc)\n",
    "# fit\n",
    "pipe_svc.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train_svm = pipe_svc.predict(X_train) \n",
    "y_pred_test_svm = pipe_svc.predict(X_test)\n",
    "\n",
    "r2_train_svm = r2_score(y_pred_train_svm, y_train)\n",
    "mse_train_svm = mean_squared_error(y_train, y_pred_train_svm)\n",
    "mae_train_svm = mean_absolute_error(y_train, y_pred_train_svm)\n",
    "\n",
    "print('r2 on the train set:', r2_train_svm)\n",
    "print('MAE on the train set:', mae_train_svm)\n",
    "print('MSE on the train set:', mse_train_svm)\n",
    "\n",
    "r2_test_svm = r2_score(y_pred_test_svm, y_test)\n",
    "mse_test_svm = mean_squared_error(y_test, y_pred_test_svm)\n",
    "mae_test_svm = mean_absolute_error(y_test, y_pred_test_svm)\n",
    "\n",
    "print('----------------')\n",
    "print('r2 on the test set:', r2_test_svm)\n",
    "print('MAE on the test set:', mae_test_svm)\n",
    "print('MSE on the test set:', mse_test_svm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "------------decision tree----------\nr2 on the train set: 1.0\nMAE on the train set: 0.5330920012614552\nMSE on the train set: 0.5273648371379568\n----------------\nr2 on the test set: 0.6107615716040615\nMAE on the test set: 0.43781382267441854\nMSE on the test set: 0.48602743476128873\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('------------decision tree----------')\n",
    "\n",
    "# pipeline\n",
    "pipe_tree = DecisionTreeRegressor(random_state=43)\n",
    "# fit\n",
    "pipe_tree.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train_tree = pipe_tree.predict(X_train) \n",
    "y_pred_test_tree = pipe_tree.predict(X_test)\n",
    "\n",
    "r2_train_tree = r2_score(y_pred_train_tree, y_train)\n",
    "mse_train_tree = mean_squared_error(y_train, y_pred_train)\n",
    "mae_train_tree = mean_absolute_error(y_train, y_pred_train)\n",
    "\n",
    "print('r2 on the train set:', r2_train_tree)\n",
    "print('MAE on the train set:', mae_train_tree)\n",
    "print('MSE on the train set:', mse_train_tree)\n",
    "\n",
    "r2_test_tree = r2_score(y_pred_test_tree, y_test)\n",
    "mse_test_tree = mean_squared_error(y_test, y_pred_test_tree)\n",
    "mae_test_tree = mean_absolute_error(y_test, y_pred_test_tree)\n",
    "\n",
    "print('----------------')\n",
    "print('r2 on the test set:', r2_test_tree)\n",
    "print('MAE on the test set:', mae_test_tree)\n",
    "print('MSE on the test set:', mse_test_tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-------------random forest------------\n",
      "r2 on the train set: 0.9705828237104931\n",
      "MAE on the train set: 0.11990197445628809\n",
      "MSE on the train set: 0.034488233047398974\n",
      "----------------\n",
      "r2 on the test set: 0.7486758098300759\n",
      "MAE on the test set: 0.32004346957364355\n",
      "MSE on the test set: 0.24226863716491928\n"
     ]
    }
   ],
   "source": [
    "print('-------------random forest------------')\n",
    "pipe_rand = RandomForestRegressor(random_state=43)\n",
    "# fit\n",
    "pipe_rand.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train_rand = pipe_rand.predict(X_train) \n",
    "y_pred_test_rand = pipe_rand.predict(X_test)\n",
    "\n",
    "r2_train_rand = r2_score(y_pred_train_rand, y_train)\n",
    "mse_train_rand = mean_squared_error(y_train, y_pred_train_rand)\n",
    "mae_train_rand = mean_absolute_error(y_train, y_pred_train_rand)\n",
    "\n",
    "print('r2 on the train set:', r2_train_rand)\n",
    "print('MAE on the train set:', mae_train_rand)\n",
    "print('MSE on the train set:', mse_train_rand)\n",
    "\n",
    "r2_test_rand = r2_score(y_pred_test_rand, y_test)\n",
    "mse_test_rand = mean_squared_error(y_test, y_pred_test_rand)\n",
    "mae_test_rand = mean_absolute_error(y_test, y_pred_test_rand)\n",
    "\n",
    "print('----------------')\n",
    "print('r2 on the test set:', r2_test_rand)\n",
    "print('MAE on the test set:', mae_test_rand)\n",
    "print('MSE on the test set:', mse_test_rand)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-------------gradient boosting------------\n",
      "r2 on the train set: 0.7395782392433273\n",
      "MAE on the train set: 0.35656543036682264\n",
      "MSE on the train set: 0.26167490389525294\n",
      "----------------\n",
      "r2 on the test set: 0.7157292841624554\n",
      "MAE on the test set: 0.36457603328113886\n",
      "MSE on the test set: 0.27058520173725403\n"
     ]
    }
   ],
   "source": [
    "print('-------------gradient boosting------------')\n",
    "pipe_grad = GradientBoostingRegressor(random_state=43)\n",
    "# fit\n",
    "pipe_grad.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train_grad = pipe_grad.predict(X_train) \n",
    "y_pred_test_grad = pipe_grad.predict(X_test)\n",
    "\n",
    "r2_train_grad = r2_score(y_pred_train_grad, y_train)\n",
    "mse_train_grad = mean_squared_error(y_train, y_pred_train_grad)\n",
    "mae_train_grad = mean_absolute_error(y_train, y_pred_train_grad)\n",
    "\n",
    "print('r2 on the train set:', r2_train_grad)\n",
    "print('MAE on the train set:', mae_train_grad)\n",
    "print('MSE on the train set:', mse_train_grad)\n",
    "\n",
    "r2_test_grad = r2_score(y_pred_test_grad, y_test)\n",
    "mse_test_grad = mean_squared_error(y_test, y_pred_test_grad)\n",
    "mae_test_grad = mean_absolute_error(y_test, y_pred_test_grad)\n",
    "\n",
    "print('----------------')\n",
    "print('r2 on the test set:', r2_test_grad)\n",
    "print('MAE on the test set:', mae_test_grad)\n",
    "print('MSE on the test set:', mse_test_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-37-0ef7bbf4246d>, line 2)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-37-0ef7bbf4246d>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    It is important to notice that the Decision Tree over fits very easily. It learns easily the training data but is not able to extrapolate on the test set. This algorithm is not used a lot.\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# It is important to notice that the Decision Tree over fits very easily. It learns easily the training data but is not able to extrapolate on the test set. This algorithm is not used a lot.\n",
    "\n",
    "# However, Random Forest and Gradient Boosting propose a solid approach to correct the over fitting (in that case the parameters `max_depth` is set to None that is why the Random Forest over fits the data). These two algorithms are used intensively in Machine Learning Projects.\n"
   ]
  }
 ]
}