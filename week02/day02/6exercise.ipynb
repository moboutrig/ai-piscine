{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python369jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6",
   "display_name": "Python 3.6.9 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "\n",
    "# Exercise 6 Multi-class (Optional)\n",
    "\n",
    "The goal of this exercise is to learn to train a classification algorithm on a multi-class labelled data.\n",
    "Some algorithms as SVM or Logistic Regression do not natively support multi-class (more than 2 classes). There are some approaches that allow to use these algorithms on multi-class data.\n",
    "Let's assume we work with 3 classes: A, B and C.\n",
    "\n",
    "- One-vs-Rest considers 3 binary classification problems: A vs B,C; B vs A,C and C vs A,B. If there are 10 classes, 10 binary classification problems would be fitted.\n",
    "- One-vs-One considers 3 binary classification problems: A vs B, A vs C, B vs C. If there are 10 classes, 45 binary classification problems would be fitted. Given, the volume of data, this technique may not be scalable.\n",
    "\n",
    "More details:\n",
    "\n",
    "- https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/\n",
    "\n",
    "Let's implement the One-vs-Rest approach from `LogisticRegression`.\n",
    "\n",
    "Preliminary:\n",
    "\n",
    "- Import the Setosa data set from Scikit-learn\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "X = pd.DataFrame(data=iris['data'], columns=iris.feature_names)\n",
    "y = pd.DataFrame(data=iris['target'], columns=['target'])\n",
    "```\n",
    "\n",
    "- Using train_test_split, split the data set in a train set and test set (20%) with `shuffle=True` and `random_state=43`.\n",
    "\n",
    "1. Create a function that takes as input the data and returns three **trained** classfiers.\n",
    "\n",
    "- `clf0` takes as input a binary data set where the class 1 is `0` and class 0 is `1` and `2`.\n",
    "- `clf1` takes as input a binary data set where the class 1 is `1` and class 0 is `0` and `2`.\n",
    "- `clf2` takes as input a binary data set where the class 1 is `2` and class 0 is `0` and `1`.\n",
    "\n",
    "```python\n",
    "def train(X_train,y_train):\n",
    "       #TODO\n",
    "       return clf0, clf1, clf2\n",
    "       \n",
    "```\n",
    "\n",
    "2. Create a function that takes as input the trained classifiers and the features set and that returns the predicted class. Use `predict_one_vs_all` to output the predicted classes on the test set. Compare the results with Logistic Regression algorithm from scikit learn used in One-vs-All mode. The results may change because the solver may not converge. Later this week, we will learn to preprocess the data to avoid convergence issues.\n",
    "\n",
    "- `clf0` outputs the probability to belong to the class 1 which is `0`.\n",
    "- `clf1` outputs the probability to belong to the class 1 which is `1`\n",
    "- `clf2` outputs the probability to belong to the class 1 which is `2`\n",
    "\n",
    "  The predicted class is the one that gets the **highest probability** among the three models.\n",
    "\n",
    "```python\n",
    "def predict_one_vs_all(X, clf0, clf1, clf2 ):\n",
    "       #TODO\n",
    "       return classes\n",
    "```\n",
    "\n",
    "- https://randerson112358.medium.com/python-logistic-regression-program-5e1b32f964db\n",
    "\n",
    "- https://towardsdatascience.com/logistic-regression-using-python-sklearn-numpy-mnist-handwriting-recognition-matplotlib-a6b31e2b166a\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X = pd.DataFrame(data=iris['data'], columns=iris.feature_names)\n",
    "y = pd.DataFrame(data=iris['target'], columns=['target'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=43, shuffle=True)\n",
    "\n",
    "# 1.\n",
    "def train(X_train, y_train):\n",
    "       clf = LogisticRegression()\n",
    "       clf1 = LogisticRegression()\n",
    "       clf2 = LogisticRegression()\n",
    "       \n",
    "       clf.fit(X_train, y_train == 0)\n",
    "       clf1.fit(X_train, y_train == 1)\n",
    "       clf2.fit(X_train, y_train == 2)\n",
    "       \n",
    "       return clf, clf1, clf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one_vs_all(X, clf0, clf1, clf2 ):\n",
    "       #TODO\n",
    "       return classes\n",
    "\n"
   ]
  }
 ]
}