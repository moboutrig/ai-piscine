{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python369jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6",
   "display_name": "Python 3.6.9 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Exercise 4 Sentences' similarity\n",
    "\n",
    "The goal of this exercise is to learn to compute the similarity between two sentences. As explained in the documentation: **The word embedding of a full sentence is simply the average over all different words**. This is how `similarity` works in SpaCy. This small use case is very interesting because if we build a corpus of sentences that express an intention as **buy shoes**, then we can detect this intention and use it to propose shoes advertisement for customers. The language model used in this exercise is `en_core_web_sm`.\n",
    "\n",
    "1. Compute the similarities (3 in total) between these sentences:\n",
    "\n",
    "```python\n",
    "sentence_1 = \"I want to buy shoes\"\n",
    "sentence_2 = \"I would love to purchase running shoes\"\n",
    "sentence_3 = \"I am in my room\"\n",
    "```\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "sentence_1 <=> sentence_2 0.6648934705292624\n",
      "sentence_1 <=> sentence_3 0.4179912927156658\n",
      "sentence_2 <=> sentence_3 0.2956368440588095\n",
      "ipykernel_launcher:15: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "ipykernel_launcher:16: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "ipykernel_launcher:17: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from scipy.spatial import distance\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "sentence_1 = \"I want to buy shoes\"\n",
    "sentence_2 = \"I would love to purchase running shoes\"\n",
    "sentence_3 = \"I am in my room\"\n",
    "\n",
    "# 1.\n",
    "doc1 = nlp(sentence_1)\n",
    "doc2 = nlp(sentence_2)\n",
    "doc3 = nlp(sentence_3)\n",
    "\n",
    "print('sentence_1 <=> sentence_2', doc1.similarity(doc2))\n",
    "print('sentence_1 <=> sentence_3', doc1.similarity(doc3))\n",
    "print('sentence_2 <=> sentence_3', doc2.similarity(doc3))\n"
   ]
  }
 ]
}